{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_representations.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNAN3oQIYu/wI2hsveu3ylD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trunghachi/GreenFlower/blob/master/word_representations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKWcgj1nJBRD"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6s7juCMxJYIr"
      },
      "source": [
        "Word Representation from https://phamdinhkhanh.github.io/2019/04/29/ModelWord2Vec.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJvwT_3MHEJd",
        "outputId": "ed1cbdeb-8692-492a-e3c6-b93286de41ce"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "\r\n",
        "le = LabelEncoder()\r\n",
        "words = ['anh', 'em', 'gia đình', 'bạn bè', 'anh', 'em']\r\n",
        "le.fit(words)\r\n",
        "\r\n",
        "print('Class of words: ', le.classes_)\r\n",
        "# Biến đổi sang dạng số\r\n",
        "x = le.transform(words)\r\n",
        "print('Convert to number: ', x)\r\n",
        "# Biến đổi lại sang class\r\n",
        "print('Invert into classes: ', le.inverse_transform(x))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Class of words:  ['anh' 'bạn bè' 'em' 'gia đình']\n",
            "Convert to number:  [0 2 3 1 0 2]\n",
            "Invert into classes:  ['anh' 'em' 'gia đình' 'bạn bè' 'anh' 'em']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wk2H3CcEIHIl",
        "outputId": "87c2acb0-9444-4acf-c099-9ff6c22bf8bb"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "oh = OneHotEncoder()\r\n",
        "classes_indices = list(zip(le.classes_, np.arange(len(le.classes_))))\r\n",
        "print('Classes_indices: ', classes_indices)\r\n",
        "oh.fit(classes_indices)\r\n",
        "print('One-hot categories and indices:', oh.categories_)\r\n",
        "# Biến đổi list words sang dạng one-hot\r\n",
        "words_indices = list(zip(words, x))\r\n",
        "print('Words and corresponding indices: ', words_indices)\r\n",
        "one_hot = oh.transform(words_indices).toarray()\r\n",
        "print('Transform words into one-hot matrices: \\n', one_hot)\r\n",
        "print('Inverse transform to categories from one-hot matrices: \\n', oh.inverse_transform(one_hot))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classes_indices:  [('anh', 0), ('bạn bè', 1), ('em', 2), ('gia đình', 3)]\n",
            "One-hot categories and indices: [array(['anh', 'bạn bè', 'em', 'gia đình'], dtype=object), array([0, 1, 2, 3], dtype=object)]\n",
            "Words and corresponding indices:  [('anh', 0), ('em', 2), ('gia đình', 3), ('bạn bè', 1), ('anh', 0), ('em', 2)]\n",
            "Transform words into one-hot matrices: \n",
            " [[1. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0. 1. 0. 0.]\n",
            " [1. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 1. 0.]]\n",
            "Inverse transform to categories from one-hot matrices: \n",
            " [['anh' 0]\n",
            " ['em' 2]\n",
            " ['gia đình' 3]\n",
            " ['bạn bè' 1]\n",
            " ['anh' 0]\n",
            " ['em' 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Grrmk73sJ-6e",
        "outputId": "f87d65c1-8912-4822-e87a-3680d9cde14d"
      },
      "source": [
        "!pip install underthesea"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting underthesea\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a8/5f/03ab9091b88e7851aa92da33f8eea6f111423cc1194cf1636c63c1fff3d0/underthesea-1.3.1-py3-none-any.whl (7.5MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5MB 4.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.0.1)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from underthesea) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from underthesea) (2.23.0)\n",
            "Collecting transformers<=3.5.1,>=3.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/83/e74092e7f24a08d751aa59b37a9fc572b2e4af3918cb66f7766c3affb1b4/transformers-3.5.1-py3-none-any.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 44.3MB/s \n",
            "\u001b[?25hCollecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 4.8MB/s \n",
            "\u001b[?25hCollecting python-crfsuite>=0.9.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/47/58f16c46506139f17de4630dbcfb877ce41a6355a1bbf3c443edb9708429/python_crfsuite-0.9.7-cp37-cp37m-manylinux1_x86_64.whl (743kB)\n",
            "\u001b[K     |████████████████████████████████| 747kB 44.6MB/s \n",
            "\u001b[?25hCollecting unidecode\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/25/723487ca2a52ebcee88a34d7d1f5a4b80b793f179ee0f62d5371938dfa01/Unidecode-1.2.0-py2.py3-none-any.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 42.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.2.5)\n",
            "Collecting torch<=1.5.1,>=1.1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/cf/007b6de316c9f3d4cb315a60c308342cc299e464167f5ebc369e93b5e23a/torch-1.5.1-cp37-cp37m-manylinux1_x86_64.whl (753.2MB)\n",
            "\u001b[K     |████████████████████████████████| 753.2MB 21kB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from underthesea) (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from underthesea) (4.41.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.13)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2020.12.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (3.0.12)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (1.19.5)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (3.12.4)\n",
            "Collecting sentencepiece==0.1.91\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f2/e2/813dff3d72df2f49554204e7e5f73a3dc0f0eb1e3958a4cad3ef3fb278b7/sentencepiece-0.1.91-cp37-cp37m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 40.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (2019.12.20)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 39.2MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.9.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/ac/f5ba028f0f097d855e1541301e946d4672eb0f30b6e25cb2369075f916d2/tokenizers-0.9.3-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 39.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers<=3.5.1,>=3.5.0->underthesea) (20.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->underthesea) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch<=1.5.1,>=1.1.0->underthesea) (0.16.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->transformers<=3.5.1,>=3.5.0->underthesea) (54.0.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers<=3.5.1,>=3.5.0->underthesea) (2.4.7)\n",
            "Building wheels for collected packages: seqeval, sacremoses\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16172 sha256=41e4da1f0377413c05a43e3e3368d9b85a57186aa844a0cabb5dbd42f619e768\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=2c4a8ea38dc27f4d8c7bd1394059a3939863d17c39411e3092395ea104be5244\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built seqeval sacremoses\n",
            "\u001b[31mERROR: torchvision 0.9.0+cu101 has requirement torch==1.8.0, but you'll have torch 1.5.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.0 has requirement torch==1.8.0, but you'll have torch 1.5.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: sentencepiece, sacremoses, tokenizers, transformers, seqeval, python-crfsuite, unidecode, torch, underthesea\n",
            "  Found existing installation: torch 1.8.0+cu101\n",
            "    Uninstalling torch-1.8.0+cu101:\n",
            "      Successfully uninstalled torch-1.8.0+cu101\n",
            "Successfully installed python-crfsuite-0.9.7 sacremoses-0.0.43 sentencepiece-0.1.91 seqeval-1.2.2 tokenizers-0.9.3 torch-1.5.1 transformers-3.5.1 underthesea-1.3.1 unidecode-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6vu2ZKuJedL",
        "outputId": "c61773cb-939c-4efa-9491-934403029f18"
      },
      "source": [
        "import scipy.linalg as ln \r\n",
        "import numpy as np\r\n",
        "from underthesea import word_tokenize\r\n",
        "\r\n",
        "sentence = 'Khoa học dữ liệu là một lĩnh vực đòi hỏi kiến thức về toán và lập trình. Tôi rất yêu thích Khoa học dữ liệu.'\r\n",
        "token = word_tokenize(sentence)\r\n",
        "# Tokenize câu search\r\n",
        "print('tokenization of sentences: ', token)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tokenization of sentences:  ['Khoa học', 'dữ liệu', 'là', 'một', 'lĩnh vực', 'đòi hỏi', 'kiến thức', 'về', 'toán', 'và', 'lập trình', '.', 'Tôi', 'rất', 'yêu thích', 'Khoa học', 'dữ liệu', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q8AD0LIxMg0k",
        "outputId": "002283ba-614e-4dc4-d37b-a73a72965d5e"
      },
      "source": [
        "from scipy.sparse import coo_matrix\r\n",
        "# Tạo ma trận coherence dưới dạng sparse thông qua khai báo vị trí khác 0 của trục x và y\r\n",
        "row = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11, 12, 13]\r\n",
        "col = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 13, 14]\r\n",
        "data =      [2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\r\n",
        "\r\n",
        "X = coo_matrix((data, (row, col)), shape=(15, 15)).toarray()\r\n",
        "print(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0 2 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jxl1W5DpMvd8",
        "outputId": "ec7dbc41-94a5-40dd-902e-08b4b8ce13ce"
      },
      "source": [
        "# Thực hiện phân tích suy biến:\r\n",
        "U, S_diag, V = ln.svd(X)\r\n",
        "print('Shape of U: ', U.shape)\r\n",
        "print('Length of diagonal: ', len(S_diag))\r\n",
        "print('Shape of V: ', V.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of U:  (15, 15)\n",
            "Length of diagonal:  15\n",
            "Shape of V:  (15, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXhHB7SAM4Lj",
        "outputId": "ca41333f-a84b-443a-ccb2-7a729b14ff4a"
      },
      "source": [
        "import numpy as np\r\n",
        "S_truncate = np.zeros(shape = (6, 15))\r\n",
        "np.fill_diagonal(S_truncate, S_diag[:6])\r\n",
        "print('S truncate: \\n', S_truncate)\r\n",
        "print('Word Embedding 6 dimensionality: \\n', np.dot(S_truncate, V))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "S truncate: \n",
            " [[2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
            "Word Embedding 6 dimensionality: \n",
            " [[0. 2. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KHrILETNCiF",
        "outputId": "757130f9-a533-48f5-f0de-dc2d37d7d7cf"
      },
      "source": [
        "from keras.layers import Dense, Input\r\n",
        "from keras.models import Model, Sequential\r\n",
        "from keras.optimizers import RMSprop, Adam\r\n",
        "\r\n",
        "def autoencoder(input_unit, hidden_unit):\r\n",
        "    model = Sequential()\r\n",
        "    model.add(Dense(input_unit, input_shape = (15,), activation = 'relu'))\r\n",
        "    model.add(Dense(hidden_unit, activation = 'relu'))\r\n",
        "    model.add(Dense(input_unit, activation = 'softmax'))\r\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer = Adam(),\r\n",
        "                 metrics = ['accuracy'])\r\n",
        "    model.summary()\r\n",
        "    return model\r\n",
        "\r\n",
        "model_auto = autoencoder(input_unit = 15, hidden_unit = 6)\r\n",
        "\r\n",
        "model_auto.fit(X, X, epochs = 5, batch_size = 3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 15)                240       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 6)                 96        \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 15)                105       \n",
            "=================================================================\n",
            "Total params: 441\n",
            "Trainable params: 441\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "5/5 [==============================] - 1s 3ms/step - loss: 2.8387 - accuracy: 0.0222\n",
            "Epoch 2/5\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 2.3578 - accuracy: 0.1380\n",
            "Epoch 3/5\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 2.3124 - accuracy: 0.0222\n",
            "Epoch 4/5\n",
            "5/5 [==============================] - 0s 2ms/step - loss: 2.6806 - accuracy: 0.0361\n",
            "Epoch 5/5\n",
            "5/5 [==============================] - 0s 3ms/step - loss: 2.4866 - accuracy: 0.0546\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f54c110a910>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSbBTN2aOn32",
        "outputId": "fa7e6324-43e3-4df0-e79c-4d3de43279bf"
      },
      "source": [
        "embedding_matrix = model_auto.layers[2].get_weights()[0]\r\n",
        "bias = model_auto.layers[2].get_weights()[1]\r\n",
        "\r\n",
        "print('Shape of embedding_matrix: ', embedding_matrix.shape)\r\n",
        "print('Embedding_matrix: \\n', embedding_matrix)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of embedding_matrix:  (6, 15)\n",
            "Embedding_matrix: \n",
            " [[-4.5710385e-01  1.3637660e-01  3.7350875e-01  3.4085038e-01\n",
            "   4.7826535e-01 -2.4833240e-02  3.1228599e-01  4.0247890e-01\n",
            "  -3.5115249e-02 -1.4893259e-01  2.5607732e-01  3.1063601e-04\n",
            "  -4.7888541e-01  5.1786691e-01 -4.4234085e-01]\n",
            " [-8.8111863e-02 -4.4486621e-01  1.7102020e-02  5.0705737e-01\n",
            "  -1.2758511e-01  4.9646223e-01 -4.5703924e-01 -3.9525250e-01\n",
            "   9.7152032e-02 -1.6107951e-01 -3.6072952e-01 -1.7451541e-02\n",
            "  -2.3617947e-01 -3.2011402e-01 -2.7804896e-01]\n",
            " [ 2.2994721e-01  4.3530560e-01 -4.1886833e-01  4.5217678e-01\n",
            "   3.5582727e-01 -4.1541064e-01 -8.5472845e-02  5.0418812e-01\n",
            "   1.9523335e-01  5.1320767e-01  3.0193558e-01 -7.9136342e-03\n",
            "   1.0971300e-01 -1.1650929e-01 -1.7923751e-01]\n",
            " [-2.9922342e-01 -3.7155688e-01  1.9838841e-01 -6.4846747e-02\n",
            "  -3.3185798e-01  3.4755009e-01  2.1325352e-02  1.6160752e-01\n",
            "   5.2371579e-01 -3.2111749e-03  3.4440181e-01 -3.8893864e-01\n",
            "  -3.2466993e-01  1.3111177e-01  4.2463216e-01]\n",
            " [ 6.6339318e-03  7.2431974e-02  4.5501059e-01 -3.6570531e-01\n",
            "  -3.4602687e-01  1.5115328e-01 -2.5655988e-01  4.2240951e-02\n",
            "  -1.5911578e-01 -4.2887643e-01  3.0095953e-01 -3.7605208e-01\n",
            "  -3.2965371e-01  3.1814685e-01  3.8201809e-01]\n",
            " [ 1.4530663e-01  3.9439622e-01  3.2847467e-01 -4.7867516e-01\n",
            "  -4.4371423e-01 -4.8317966e-01  4.6337056e-01  3.6633289e-01\n",
            "   4.1702804e-01  1.5260655e-01  4.4996846e-01 -3.0106273e-01\n",
            "   1.3289623e-01 -2.3102438e-01 -1.8964337e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVmA4bQlOxdM",
        "outputId": "316324a6-38a9-4450-cbf7-8e5e4a85bfd9"
      },
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\r\n",
        "from numpy.linalg import norm\r\n",
        "\r\n",
        "def cosine(x, y):\r\n",
        "    cos_sim = np.dot(x, y)/(norm(x)*norm(y))\r\n",
        "    return cos_sim\r\n",
        "# Véc tơ biểu diễn từ khoa học\r\n",
        "e0 = list(embedding_matrix[:, 0])\r\n",
        "# Véc tơ biểu diễn từ dữ liệu\r\n",
        "e1 = list(embedding_matrix[:, 1])\r\n",
        "# Quan hệ tương quan ngữ nghĩa giữa từ khoa học và dữ liệu\r\n",
        "cosine(e0, e1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.47501302"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZbflswdMO37U",
        "outputId": "85d3accb-6d1d-486f-e87a-b83c0588fb54"
      },
      "source": [
        "# Từ có khoảng cách lớn nhất với từ khoa học theo thứ tự\r\n",
        "cosines = [cosine(e0, embedding_matrix[:, i]) for i in np.arange(15)]\r\n",
        "print('cosines: ', cosines)\r\n",
        "np.argsort([cosine(e0, embedding_matrix[:, i]) for i in np.arange(15)])[::-1]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cosines:  [1.0, 0.47501302, -0.5493009, -0.24871396, -0.16882728, -0.5470442, -0.13223119, -0.05308816, -0.10048421, 0.4966199, -0.09994887, 0.18250734, 0.84556746, -0.6654595, 0.066298455]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0, 12,  9,  1, 11, 14,  7, 10,  8,  6,  4,  3,  5,  2, 13])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oepy0lnVPjik"
      },
      "source": [
        "# Word2vec\r\n",
        "\r\n",
        "## Bước 1: Tạo từ điển"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EC8JERmYPP60",
        "outputId": "3d3fd975-33fd-4793-e8a0-20f74adbb838"
      },
      "source": [
        "from keras.preprocessing import text\r\n",
        "from keras.utils import np_utils\r\n",
        "from keras.preprocessing import sequence\r\n",
        "from nltk.corpus import gutenberg\r\n",
        "from string import punctuation\r\n",
        "import nltk\r\n",
        "nltk.download('gutenberg')\r\n",
        "nltk.download('punkt')\r\n",
        "norm_bible = gutenberg.sents('bible-kjv.txt') \r\n",
        "norm_bible = [' '.join(doc) for doc in norm_bible]\r\n",
        "tokenizer = text.Tokenizer()\r\n",
        "tokenizer.fit_on_texts(norm_bible)\r\n",
        "word2id = tokenizer.word_index\r\n",
        "\r\n",
        "# build vocabulary of unique words\r\n",
        "word2id['PAD'] = 0\r\n",
        "id2word = {v:k for k, v in word2id.items()}\r\n",
        "vocab_size = len(word2id)\r\n",
        "\r\n",
        "print('Vocabulary Size:', vocab_size)\r\n",
        "print('Vocabulary Sample:', list(word2id.items())[:10])\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Vocabulary Size: 12746\n",
            "Vocabulary Sample: [('the', 1), ('and', 2), ('of', 3), ('to', 4), ('that', 5), ('in', 6), ('he', 7), ('shall', 8), ('unto', 9), ('for', 10)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWT3LHUJPzNF"
      },
      "source": [
        "## Bước 2: Mã hoá toàn bộ các câu văn bằng index."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Tr3WQZ7PYys",
        "outputId": "d28f1b41-efee-4122-d998-0038df794b12"
      },
      "source": [
        "wids = [[word2id[w] for w in text.text_to_word_sequence(doc)] for doc in norm_bible]\r\n",
        "print('Embedding sentence by index: ', wids[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Embedding sentence by index:  [[1, 53, 1342, 6058], [1, 280, 2678, 3, 1, 53, 1342, 6058], [1, 254, 448, 3, 162, 194, 8769], [43, 43, 6, 1, 734, 27, 1368, 1, 205, 2, 1, 139], [43, 48, 2, 1, 139, 26, 258, 2085, 2, 2086, 2, 551, 26, 46, 1, 266, 3, 1, 1030]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eztpr_9QV3E"
      },
      "source": [
        "## Bước 3: Xác định Context --> Target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XP_XgftNQKr3",
        "outputId": "91a04e91-59b4-4f7e-c185-b95c39fe0341"
      },
      "source": [
        "import numpy as np\r\n",
        "def generate_context_word_pairs(corpus, window_size, vocab_size):\r\n",
        "    context_length = window_size*2\r\n",
        "    for words in corpus:\r\n",
        "        sentence_length = len(words)\r\n",
        "        # print('words: ', words)\r\n",
        "        for index, word in enumerate(words):\r\n",
        "            context_words = []\r\n",
        "            label_word   = [] \r\n",
        "            # Start index of context\r\n",
        "            start = index - window_size\r\n",
        "            # End index of context\r\n",
        "            end = index + window_size + 1\r\n",
        "            # List of context_words\r\n",
        "            context_words.append([words[i] for i in range(start, end) if 0 <= i < sentence_length and i != index])\r\n",
        "            # List of label_word (also is target word).\r\n",
        "            # print('context words {}: {}'.format(context_words, index))\r\n",
        "            label_word.append(word)\r\n",
        "            # Padding the input 0 in the left in case it does not satisfy number of context_words = 2*window_size.\r\n",
        "            x = sequence.pad_sequences(context_words, maxlen=context_length)\r\n",
        "            # print('context words padded: ', x)\r\n",
        "            # Convert label_word into one-hot vector corresponding with its index\r\n",
        "            y = np_utils.to_categorical(label_word, vocab_size)\r\n",
        "            yield (x, y)\r\n",
        "            \r\n",
        "            \r\n",
        "# Test this out for some samples\r\n",
        "i = 0\r\n",
        "window_size = 2 # context window size\r\n",
        "for x, y in generate_context_word_pairs(corpus=wids, window_size=window_size, vocab_size=vocab_size):\r\n",
        "    if 0 not in x[0]:\r\n",
        "        print('Context (X):', [id2word[w] for w in x[0]], '-> Target (Y):', id2word[np.argwhere(y[0])[0][0]])\r\n",
        "    \r\n",
        "        if i == 10:\r\n",
        "            break\r\n",
        "        i += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Context (X): ['the', 'old', 'of', 'the'] -> Target (Y): testament\n",
            "Context (X): ['old', 'testament', 'the', 'king'] -> Target (Y): of\n",
            "Context (X): ['testament', 'of', 'king', 'james'] -> Target (Y): the\n",
            "Context (X): ['of', 'the', 'james', 'bible'] -> Target (Y): king\n",
            "Context (X): ['the', 'first', 'of', 'moses'] -> Target (Y): book\n",
            "Context (X): ['first', 'book', 'moses', 'called'] -> Target (Y): of\n",
            "Context (X): ['book', 'of', 'called', 'genesis'] -> Target (Y): moses\n",
            "Context (X): ['1', '1', 'the', 'beginning'] -> Target (Y): in\n",
            "Context (X): ['1', 'in', 'beginning', 'god'] -> Target (Y): the\n",
            "Context (X): ['in', 'the', 'god', 'created'] -> Target (Y): beginning\n",
            "Context (X): ['the', 'beginning', 'created', 'the'] -> Target (Y): god\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jXa5RkQQ5Dt"
      },
      "source": [
        "## Bước 4: Xây dựng mạng nơ ron gồm 3 layers chính:\r\n",
        "\r\n",
        "Embedding layer: dùng để mã hoá đầu vào thành các one-hot véc tơ. Số lượng từ ở đầu vào chính là 2*window_size. Sau khi mã hoá, qua quá trình training mỗi một từ vựng sẽ được biểu diễn bởi một véc tơ nhúng 100 chiều tương ứng với embed_size.\r\n",
        "Mean layer: Tính véc tơ trung bình của các véc tơ đầu ra ở Embedding layer. Số lượng véc tơ là 2*window_size.\r\n",
        "Dense layer: Tính phân phối xác xuất của từ Target dựa vào hàm softmax."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_X9mOcNBQut7",
        "outputId": "061f930b-64c1-496f-cf33-86c212a369ca"
      },
      "source": [
        "import keras.backend as K\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.layers import Dense, Embedding, Lambda\r\n",
        "embed_size = 100\r\n",
        "\r\n",
        "# build CBOW architecture\r\n",
        "cbow = Sequential()\r\n",
        "cbow.add(Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=window_size*2))\r\n",
        "cbow.add(Lambda(lambda x: K.mean(x, axis=1), output_shape=(embed_size,)))\r\n",
        "cbow.add(Dense(vocab_size, activation='softmax'))\r\n",
        "cbow.compile(loss='categorical_crossentropy', optimizer='rmsprop')\r\n",
        "\r\n",
        "# view model summary\r\n",
        "print(cbow.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 4, 100)            1274600   \n",
            "_________________________________________________________________\n",
            "lambda (Lambda)              (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 12746)             1287346   \n",
            "=================================================================\n",
            "Total params: 2,561,946\n",
            "Trainable params: 2,561,946\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YRF2j-KjRQKo",
        "outputId": "2cefe3ab-837a-4c4e-e764-993fb8d43b98"
      },
      "source": [
        "print('number of window: ', len(wids))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of window:  30103\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VgPYcRcRU7d"
      },
      "source": [
        "## Bước 5: Huấn luyện mô hình. \r\n",
        "Chúng ta sẽ huấn luyện mô hình dựa trên 100 câu văn đầu tiên và trải qua 5 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSgGWtbkRZiF",
        "outputId": "38620362-7b5e-4e1b-ad7d-0dbb5adf8eb0"
      },
      "source": [
        "for epoch in range(1, 6):\r\n",
        "    loss = 0.\r\n",
        "    i = 0\r\n",
        "    for x, y in generate_context_word_pairs(corpus=wids[:100], window_size=window_size, vocab_size=vocab_size):\r\n",
        "        i += 1\r\n",
        "        loss += cbow.train_on_batch(x, y)\r\n",
        "        if i % 500 == 0:\r\n",
        "            print('Processed {} (context, word) pairs'.format(i))\r\n",
        "\r\n",
        "    print('Epoch:', epoch, '\\tLoss:', loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processed 500 (context, word) pairs\n",
            "Processed 1000 (context, word) pairs\n",
            "Processed 1500 (context, word) pairs\n",
            "Processed 2000 (context, word) pairs\n",
            "Processed 2500 (context, word) pairs\n",
            "Epoch: 1 \tLoss: 20381.339057028294\n",
            "Processed 500 (context, word) pairs\n",
            "Processed 1000 (context, word) pairs\n",
            "Processed 1500 (context, word) pairs\n",
            "Processed 2000 (context, word) pairs\n",
            "Processed 2500 (context, word) pairs\n",
            "Epoch: 2 \tLoss: 17202.120052218437\n",
            "Processed 500 (context, word) pairs\n",
            "Processed 1000 (context, word) pairs\n",
            "Processed 1500 (context, word) pairs\n",
            "Processed 2000 (context, word) pairs\n",
            "Processed 2500 (context, word) pairs\n",
            "Epoch: 3 \tLoss: 16379.310061559081\n",
            "Processed 500 (context, word) pairs\n",
            "Processed 1000 (context, word) pairs\n",
            "Processed 1500 (context, word) pairs\n",
            "Processed 2000 (context, word) pairs\n",
            "Processed 2500 (context, word) pairs\n",
            "Epoch: 4 \tLoss: 15567.846531778574\n",
            "Processed 500 (context, word) pairs\n",
            "Processed 1000 (context, word) pairs\n",
            "Processed 1500 (context, word) pairs\n",
            "Processed 2000 (context, word) pairs\n",
            "Processed 2500 (context, word) pairs\n",
            "Epoch: 5 \tLoss: 15094.578967623413\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mgl5wtazSwZ9"
      },
      "source": [
        "## Bước 6: Trích xuất ma trận nhúng của các từ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "RknurA4nS301",
        "outputId": "ff8177a3-6fa0-49ef-9207-a0921583b9f7"
      },
      "source": [
        "import pandas as pd\r\n",
        "weights = cbow.get_weights()[0]\r\n",
        "weights = weights[1:]\r\n",
        "print(weights.shape)\r\n",
        "\r\n",
        "pd.DataFrame(weights, index=list(id2word.values())[1:]).head()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12745, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>-0.174165</td>\n",
              "      <td>-0.063822</td>\n",
              "      <td>0.305682</td>\n",
              "      <td>-0.129642</td>\n",
              "      <td>-0.474484</td>\n",
              "      <td>0.109544</td>\n",
              "      <td>0.291508</td>\n",
              "      <td>-0.625555</td>\n",
              "      <td>-0.319643</td>\n",
              "      <td>-0.146942</td>\n",
              "      <td>0.527215</td>\n",
              "      <td>-0.363833</td>\n",
              "      <td>-0.168716</td>\n",
              "      <td>0.009517</td>\n",
              "      <td>-0.321934</td>\n",
              "      <td>-0.331385</td>\n",
              "      <td>-0.093987</td>\n",
              "      <td>0.045994</td>\n",
              "      <td>-0.045901</td>\n",
              "      <td>0.268133</td>\n",
              "      <td>-0.213596</td>\n",
              "      <td>0.066872</td>\n",
              "      <td>-0.481769</td>\n",
              "      <td>0.069016</td>\n",
              "      <td>-0.290829</td>\n",
              "      <td>0.176083</td>\n",
              "      <td>0.047188</td>\n",
              "      <td>-0.172513</td>\n",
              "      <td>-0.124460</td>\n",
              "      <td>0.258850</td>\n",
              "      <td>0.151776</td>\n",
              "      <td>0.056428</td>\n",
              "      <td>0.306783</td>\n",
              "      <td>-0.234796</td>\n",
              "      <td>0.090639</td>\n",
              "      <td>-0.098114</td>\n",
              "      <td>-0.096069</td>\n",
              "      <td>0.068440</td>\n",
              "      <td>-0.307928</td>\n",
              "      <td>0.250268</td>\n",
              "      <td>...</td>\n",
              "      <td>0.136678</td>\n",
              "      <td>0.185197</td>\n",
              "      <td>-0.063017</td>\n",
              "      <td>-0.035304</td>\n",
              "      <td>0.056663</td>\n",
              "      <td>-0.006995</td>\n",
              "      <td>-0.093538</td>\n",
              "      <td>0.059816</td>\n",
              "      <td>0.319433</td>\n",
              "      <td>-0.700160</td>\n",
              "      <td>0.224535</td>\n",
              "      <td>-0.021988</td>\n",
              "      <td>0.273491</td>\n",
              "      <td>0.255238</td>\n",
              "      <td>-0.194397</td>\n",
              "      <td>-0.081870</td>\n",
              "      <td>-0.024385</td>\n",
              "      <td>0.186537</td>\n",
              "      <td>-0.109554</td>\n",
              "      <td>0.029254</td>\n",
              "      <td>0.778938</td>\n",
              "      <td>-0.608922</td>\n",
              "      <td>-0.200541</td>\n",
              "      <td>-0.271270</td>\n",
              "      <td>-0.123018</td>\n",
              "      <td>0.432078</td>\n",
              "      <td>0.154745</td>\n",
              "      <td>-0.022569</td>\n",
              "      <td>0.316961</td>\n",
              "      <td>0.261395</td>\n",
              "      <td>-0.132290</td>\n",
              "      <td>-0.098971</td>\n",
              "      <td>0.033723</td>\n",
              "      <td>0.030183</td>\n",
              "      <td>-0.200391</td>\n",
              "      <td>0.058879</td>\n",
              "      <td>0.149272</td>\n",
              "      <td>0.479433</td>\n",
              "      <td>0.204126</td>\n",
              "      <td>0.777097</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>-0.090846</td>\n",
              "      <td>-0.175164</td>\n",
              "      <td>-0.442010</td>\n",
              "      <td>-0.013573</td>\n",
              "      <td>-0.055629</td>\n",
              "      <td>0.118096</td>\n",
              "      <td>0.376133</td>\n",
              "      <td>-0.460013</td>\n",
              "      <td>-0.005129</td>\n",
              "      <td>0.022540</td>\n",
              "      <td>0.036364</td>\n",
              "      <td>-0.073816</td>\n",
              "      <td>-0.113738</td>\n",
              "      <td>0.026863</td>\n",
              "      <td>-0.463848</td>\n",
              "      <td>-0.037831</td>\n",
              "      <td>0.184645</td>\n",
              "      <td>-0.274103</td>\n",
              "      <td>0.086699</td>\n",
              "      <td>-0.162517</td>\n",
              "      <td>-0.221973</td>\n",
              "      <td>0.081224</td>\n",
              "      <td>-0.142764</td>\n",
              "      <td>0.073115</td>\n",
              "      <td>0.105382</td>\n",
              "      <td>0.095279</td>\n",
              "      <td>-0.083502</td>\n",
              "      <td>0.041074</td>\n",
              "      <td>0.089867</td>\n",
              "      <td>-0.198816</td>\n",
              "      <td>0.203450</td>\n",
              "      <td>0.133081</td>\n",
              "      <td>0.252221</td>\n",
              "      <td>-0.896949</td>\n",
              "      <td>0.086407</td>\n",
              "      <td>-0.147757</td>\n",
              "      <td>0.040708</td>\n",
              "      <td>0.622325</td>\n",
              "      <td>0.277127</td>\n",
              "      <td>0.013862</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.146326</td>\n",
              "      <td>-0.053859</td>\n",
              "      <td>0.035312</td>\n",
              "      <td>0.098230</td>\n",
              "      <td>-0.042671</td>\n",
              "      <td>-0.095430</td>\n",
              "      <td>0.112066</td>\n",
              "      <td>0.188601</td>\n",
              "      <td>0.303919</td>\n",
              "      <td>0.059489</td>\n",
              "      <td>0.687602</td>\n",
              "      <td>-0.397082</td>\n",
              "      <td>-0.066953</td>\n",
              "      <td>0.149050</td>\n",
              "      <td>-0.087039</td>\n",
              "      <td>-0.118994</td>\n",
              "      <td>-0.146032</td>\n",
              "      <td>0.748103</td>\n",
              "      <td>-0.017916</td>\n",
              "      <td>0.060508</td>\n",
              "      <td>0.130677</td>\n",
              "      <td>-0.357195</td>\n",
              "      <td>0.022824</td>\n",
              "      <td>-0.061892</td>\n",
              "      <td>-0.347332</td>\n",
              "      <td>-0.046408</td>\n",
              "      <td>0.499997</td>\n",
              "      <td>-0.039043</td>\n",
              "      <td>0.176881</td>\n",
              "      <td>0.010419</td>\n",
              "      <td>0.019872</td>\n",
              "      <td>0.180331</td>\n",
              "      <td>-0.232061</td>\n",
              "      <td>0.157660</td>\n",
              "      <td>-0.194886</td>\n",
              "      <td>-0.085244</td>\n",
              "      <td>0.154849</td>\n",
              "      <td>0.347828</td>\n",
              "      <td>0.047647</td>\n",
              "      <td>0.301733</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>-0.119218</td>\n",
              "      <td>-0.046323</td>\n",
              "      <td>-0.581406</td>\n",
              "      <td>-0.252867</td>\n",
              "      <td>-0.007689</td>\n",
              "      <td>0.064402</td>\n",
              "      <td>0.101480</td>\n",
              "      <td>-0.279862</td>\n",
              "      <td>-0.159458</td>\n",
              "      <td>-0.165170</td>\n",
              "      <td>-0.193688</td>\n",
              "      <td>-0.376131</td>\n",
              "      <td>0.156756</td>\n",
              "      <td>-0.181204</td>\n",
              "      <td>-0.641110</td>\n",
              "      <td>0.040666</td>\n",
              "      <td>-0.031761</td>\n",
              "      <td>-0.014076</td>\n",
              "      <td>-0.053418</td>\n",
              "      <td>-0.137756</td>\n",
              "      <td>0.074456</td>\n",
              "      <td>0.171095</td>\n",
              "      <td>0.114714</td>\n",
              "      <td>-0.163437</td>\n",
              "      <td>0.229654</td>\n",
              "      <td>0.033202</td>\n",
              "      <td>-0.104749</td>\n",
              "      <td>0.152151</td>\n",
              "      <td>0.149906</td>\n",
              "      <td>-0.075600</td>\n",
              "      <td>0.010086</td>\n",
              "      <td>0.528265</td>\n",
              "      <td>-0.071942</td>\n",
              "      <td>-0.462355</td>\n",
              "      <td>0.014845</td>\n",
              "      <td>-0.068399</td>\n",
              "      <td>-0.125582</td>\n",
              "      <td>-0.302177</td>\n",
              "      <td>-0.108413</td>\n",
              "      <td>-0.151451</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.036079</td>\n",
              "      <td>0.025415</td>\n",
              "      <td>-0.083480</td>\n",
              "      <td>-0.052953</td>\n",
              "      <td>-0.031651</td>\n",
              "      <td>-0.117602</td>\n",
              "      <td>0.247594</td>\n",
              "      <td>0.029516</td>\n",
              "      <td>0.111236</td>\n",
              "      <td>0.043111</td>\n",
              "      <td>0.300878</td>\n",
              "      <td>-0.153106</td>\n",
              "      <td>-0.192241</td>\n",
              "      <td>0.274321</td>\n",
              "      <td>0.006930</td>\n",
              "      <td>-0.094067</td>\n",
              "      <td>-0.160363</td>\n",
              "      <td>0.621428</td>\n",
              "      <td>0.091545</td>\n",
              "      <td>-0.221804</td>\n",
              "      <td>0.040056</td>\n",
              "      <td>0.073310</td>\n",
              "      <td>-0.152801</td>\n",
              "      <td>-0.056969</td>\n",
              "      <td>-0.029636</td>\n",
              "      <td>0.038825</td>\n",
              "      <td>0.337642</td>\n",
              "      <td>0.143470</td>\n",
              "      <td>-0.045011</td>\n",
              "      <td>-0.092308</td>\n",
              "      <td>0.123755</td>\n",
              "      <td>-0.061171</td>\n",
              "      <td>-0.211318</td>\n",
              "      <td>0.017173</td>\n",
              "      <td>-0.318532</td>\n",
              "      <td>-0.303251</td>\n",
              "      <td>0.225820</td>\n",
              "      <td>0.188696</td>\n",
              "      <td>0.152195</td>\n",
              "      <td>0.088640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>that</th>\n",
              "      <td>-0.093615</td>\n",
              "      <td>-0.032653</td>\n",
              "      <td>-0.025385</td>\n",
              "      <td>0.018768</td>\n",
              "      <td>-0.078177</td>\n",
              "      <td>0.064099</td>\n",
              "      <td>0.209154</td>\n",
              "      <td>-0.180818</td>\n",
              "      <td>-0.068139</td>\n",
              "      <td>0.103987</td>\n",
              "      <td>-0.120559</td>\n",
              "      <td>-0.006583</td>\n",
              "      <td>-0.009840</td>\n",
              "      <td>-0.070138</td>\n",
              "      <td>-0.145713</td>\n",
              "      <td>0.015771</td>\n",
              "      <td>0.044901</td>\n",
              "      <td>-0.022386</td>\n",
              "      <td>-0.030903</td>\n",
              "      <td>0.037759</td>\n",
              "      <td>-0.016769</td>\n",
              "      <td>0.005529</td>\n",
              "      <td>-0.094002</td>\n",
              "      <td>0.033058</td>\n",
              "      <td>-0.042852</td>\n",
              "      <td>0.047525</td>\n",
              "      <td>-0.088438</td>\n",
              "      <td>0.054379</td>\n",
              "      <td>0.020636</td>\n",
              "      <td>-0.077262</td>\n",
              "      <td>0.023376</td>\n",
              "      <td>0.114570</td>\n",
              "      <td>-0.007696</td>\n",
              "      <td>-0.148720</td>\n",
              "      <td>-0.021701</td>\n",
              "      <td>-0.060020</td>\n",
              "      <td>-0.032234</td>\n",
              "      <td>-0.009235</td>\n",
              "      <td>0.040473</td>\n",
              "      <td>-0.066276</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.013769</td>\n",
              "      <td>0.026570</td>\n",
              "      <td>-0.014131</td>\n",
              "      <td>0.095704</td>\n",
              "      <td>-0.015051</td>\n",
              "      <td>0.029500</td>\n",
              "      <td>0.086795</td>\n",
              "      <td>0.058553</td>\n",
              "      <td>0.103425</td>\n",
              "      <td>-0.080309</td>\n",
              "      <td>0.181327</td>\n",
              "      <td>-0.152638</td>\n",
              "      <td>-0.103660</td>\n",
              "      <td>0.088559</td>\n",
              "      <td>-0.055853</td>\n",
              "      <td>-0.055279</td>\n",
              "      <td>-0.107276</td>\n",
              "      <td>0.110441</td>\n",
              "      <td>-0.151441</td>\n",
              "      <td>-0.100677</td>\n",
              "      <td>-0.084205</td>\n",
              "      <td>0.032814</td>\n",
              "      <td>0.111858</td>\n",
              "      <td>-0.013872</td>\n",
              "      <td>-0.060110</td>\n",
              "      <td>0.057995</td>\n",
              "      <td>0.252581</td>\n",
              "      <td>-0.029353</td>\n",
              "      <td>-0.015761</td>\n",
              "      <td>-0.052892</td>\n",
              "      <td>0.035620</td>\n",
              "      <td>0.067076</td>\n",
              "      <td>-0.031382</td>\n",
              "      <td>0.053417</td>\n",
              "      <td>-0.032456</td>\n",
              "      <td>-0.083740</td>\n",
              "      <td>0.005829</td>\n",
              "      <td>0.092708</td>\n",
              "      <td>0.012419</td>\n",
              "      <td>0.066568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>in</th>\n",
              "      <td>0.127555</td>\n",
              "      <td>-0.023785</td>\n",
              "      <td>-0.001758</td>\n",
              "      <td>-0.205759</td>\n",
              "      <td>-0.084542</td>\n",
              "      <td>0.068577</td>\n",
              "      <td>0.239442</td>\n",
              "      <td>-0.238108</td>\n",
              "      <td>-0.058259</td>\n",
              "      <td>-0.037175</td>\n",
              "      <td>0.029379</td>\n",
              "      <td>-0.101301</td>\n",
              "      <td>-0.034792</td>\n",
              "      <td>-0.090113</td>\n",
              "      <td>-0.160627</td>\n",
              "      <td>-0.164604</td>\n",
              "      <td>0.099454</td>\n",
              "      <td>-0.073309</td>\n",
              "      <td>-0.001546</td>\n",
              "      <td>0.010160</td>\n",
              "      <td>0.118459</td>\n",
              "      <td>0.056896</td>\n",
              "      <td>0.025117</td>\n",
              "      <td>-0.048889</td>\n",
              "      <td>-0.001416</td>\n",
              "      <td>0.117017</td>\n",
              "      <td>-0.061175</td>\n",
              "      <td>-0.017258</td>\n",
              "      <td>-0.002963</td>\n",
              "      <td>0.143477</td>\n",
              "      <td>0.050192</td>\n",
              "      <td>0.004583</td>\n",
              "      <td>-0.019565</td>\n",
              "      <td>-0.203265</td>\n",
              "      <td>-0.161039</td>\n",
              "      <td>-0.044636</td>\n",
              "      <td>-0.060317</td>\n",
              "      <td>0.043246</td>\n",
              "      <td>-0.080028</td>\n",
              "      <td>0.082179</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.040953</td>\n",
              "      <td>0.044966</td>\n",
              "      <td>0.041482</td>\n",
              "      <td>0.121832</td>\n",
              "      <td>-0.016653</td>\n",
              "      <td>-0.072153</td>\n",
              "      <td>-0.045871</td>\n",
              "      <td>0.016922</td>\n",
              "      <td>-0.076238</td>\n",
              "      <td>0.005847</td>\n",
              "      <td>0.157454</td>\n",
              "      <td>-0.212193</td>\n",
              "      <td>-0.064430</td>\n",
              "      <td>0.084612</td>\n",
              "      <td>-0.119276</td>\n",
              "      <td>-0.061387</td>\n",
              "      <td>-0.025075</td>\n",
              "      <td>0.027538</td>\n",
              "      <td>-0.082187</td>\n",
              "      <td>-0.065353</td>\n",
              "      <td>0.082050</td>\n",
              "      <td>-0.027151</td>\n",
              "      <td>-0.010977</td>\n",
              "      <td>-0.071115</td>\n",
              "      <td>-0.097857</td>\n",
              "      <td>0.147750</td>\n",
              "      <td>0.069914</td>\n",
              "      <td>0.071602</td>\n",
              "      <td>0.107828</td>\n",
              "      <td>-0.134767</td>\n",
              "      <td>0.054662</td>\n",
              "      <td>0.119281</td>\n",
              "      <td>-0.035458</td>\n",
              "      <td>0.031103</td>\n",
              "      <td>-0.030944</td>\n",
              "      <td>-0.212545</td>\n",
              "      <td>0.025302</td>\n",
              "      <td>-0.067951</td>\n",
              "      <td>-0.025826</td>\n",
              "      <td>-0.089512</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2   ...        97        98        99\n",
              "and  -0.174165 -0.063822  0.305682  ...  0.479433  0.204126  0.777097\n",
              "of   -0.090846 -0.175164 -0.442010  ...  0.347828  0.047647  0.301733\n",
              "to   -0.119218 -0.046323 -0.581406  ...  0.188696  0.152195  0.088640\n",
              "that -0.093615 -0.032653 -0.025385  ...  0.092708  0.012419  0.066568\n",
              "in    0.127555 -0.023785 -0.001758  ... -0.067951 -0.025826 -0.089512\n",
              "\n",
              "[5 rows x 100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_j0IC4ETAUV"
      },
      "source": [
        "## Thể hiện cấu trúc của mô hình"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        },
        "id": "ZvK5w8S4TJu1",
        "outputId": "415a31a1-3ae0-41ba-d509-43dc9af5aeff"
      },
      "source": [
        "from IPython.display import SVG\r\n",
        "from keras.utils.vis_utils import model_to_dot\r\n",
        "\r\n",
        "SVG(model_to_dot(cbow, show_shapes=True, show_layer_names=False, \r\n",
        "                 rankdir='TB').create(prog='dot', format='svg'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"405pt\" viewBox=\"0.00 0.00 252.00 304.00\" width=\"336pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1.3333 1.3333) rotate(0) translate(4 300)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-300 248,-300 248,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140001941628432 -->\n<g class=\"node\" id=\"node1\">\n<title>140001941628432</title>\n<polygon fill=\"none\" points=\"12.5,-249.5 12.5,-295.5 231.5,-295.5 231.5,-249.5 12.5,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"52.5\" y=\"-268.8\">InputLayer</text>\n<polyline fill=\"none\" points=\"92.5,-249.5 92.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"121.5\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"92.5,-272.5 150.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"121.5\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"150.5,-249.5 150.5,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"191\" y=\"-280.3\">[(None, 4)]</text>\n<polyline fill=\"none\" points=\"150.5,-272.5 231.5,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"191\" y=\"-257.3\">[(None, 4)]</text>\n</g>\n<!-- 140001982988560 -->\n<g class=\"node\" id=\"node2\">\n<title>140001982988560</title>\n<polygon fill=\"none\" points=\"0,-166.5 0,-212.5 244,-212.5 244,-166.5 0,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"42\" y=\"-185.8\">Embedding</text>\n<polyline fill=\"none\" points=\"84,-166.5 84,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"113\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"84,-189.5 142,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"113\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"142,-166.5 142,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193\" y=\"-197.3\">(None, 4)</text>\n<polyline fill=\"none\" points=\"142,-189.5 244,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193\" y=\"-174.3\">(None, 4, 100)</text>\n</g>\n<!-- 140001941628432&#45;&gt;140001982988560 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140001941628432-&gt;140001982988560</title>\n<path d=\"M122,-249.3799C122,-241.1745 122,-231.7679 122,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"125.5001,-222.784 122,-212.784 118.5001,-222.784 125.5001,-222.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140002035237200 -->\n<g class=\"node\" id=\"node3\">\n<title>140002035237200</title>\n<polygon fill=\"none\" points=\"10,-83.5 10,-129.5 234,-129.5 234,-83.5 10,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"42\" y=\"-102.8\">Lambda</text>\n<polyline fill=\"none\" points=\"74,-83.5 74,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"74,-106.5 132,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"103\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"132,-83.5 132,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183\" y=\"-114.3\">(None, 4, 100)</text>\n<polyline fill=\"none\" points=\"132,-106.5 234,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"183\" y=\"-91.3\">(None, 100)</text>\n</g>\n<!-- 140001982988560&#45;&gt;140002035237200 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140001982988560-&gt;140002035237200</title>\n<path d=\"M122,-166.3799C122,-158.1745 122,-148.7679 122,-139.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"125.5001,-139.784 122,-129.784 118.5001,-139.784 125.5001,-139.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140001993181264 -->\n<g class=\"node\" id=\"node4\">\n<title>140001993181264</title>\n<polygon fill=\"none\" points=\"16,-.5 16,-46.5 228,-46.5 228,-.5 16,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"42\" y=\"-19.8\">Dense</text>\n<polyline fill=\"none\" points=\"68,-.5 68,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"68,-23.5 126,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"97\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"126,-.5 126,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177\" y=\"-31.3\">(None, 100)</text>\n<polyline fill=\"none\" points=\"126,-23.5 228,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"177\" y=\"-8.3\">(None, 12746)</text>\n</g>\n<!-- 140002035237200&#45;&gt;140001993181264 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140002035237200-&gt;140001993181264</title>\n<path d=\"M122,-83.3799C122,-75.1745 122,-65.7679 122,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"125.5001,-56.784 122,-46.784 118.5001,-56.784 125.5001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vh_2ZVNHTapW"
      },
      "source": [
        "## Hoàn toàn tương tự như kiến trúc của CBOW, ta xây dựng model skip-grams như sau:\r\n",
        "\r\n",
        "## Bước 1: Chuẩn bị dữ liệu là các cặp [context, target]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_bl8ts0uTWSj",
        "outputId": "1d437b63-0c13-4054-b430-d6e751a0f9c2"
      },
      "source": [
        "from keras.preprocessing.sequence import skipgrams\r\n",
        "\r\n",
        "# generate skip-grams\r\n",
        "skip_grams = [skipgrams(wid, vocabulary_size=vocab_size, window_size=window_size) for wid in wids[:100]]\r\n",
        "\r\n",
        "# view sample skip-grams\r\n",
        "pairs, labels = skip_grams[0][0], skip_grams[0][1]\r\n",
        "for i in range(10):\r\n",
        "    print(\"({:s} ({:d}), {:s} ({:d})) -> {:d}\".format(\r\n",
        "          id2word[pairs[i][0]], pairs[i][0], \r\n",
        "          id2word[pairs[i][1]], pairs[i][1], \r\n",
        "          labels[i]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(james (1342), occasion (2282)) -> 0\n",
            "(king (53), strip (4318)) -> 0\n",
            "(king (53), answered (232)) -> 0\n",
            "(james (1342), swaddling (8484)) -> 0\n",
            "(the (1), worse (1988)) -> 0\n",
            "(bible (6058), james (1342)) -> 1\n",
            "(king (53), bullocks (1389)) -> 0\n",
            "(james (1342), kareah (3158)) -> 0\n",
            "(bible (6058), pleiades (7936)) -> 0\n",
            "(king (53), james (1342)) -> 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE5O8Z0-T1qf"
      },
      "source": [
        "## Bước 2: Xây dựng mạng nơ ron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i2zjtDC_T6k9",
        "outputId": "ec3482be-2af8-4cec-debe-dc85653a8fc6"
      },
      "source": [
        "from keras.layers import Input, Dot, dot, concatenate\r\n",
        "# from keras.engine.input_layer import Input\r\n",
        "from keras.layers.core import Dense, Reshape\r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from keras.models import Sequential, Model, Input\r\n",
        "\r\n",
        "# build skip-gram architecture\r\n",
        "word_input = Input(shape = (1,))\r\n",
        "word_embed = Embedding(vocab_size, embed_size,\r\n",
        "                         embeddings_initializer=\"glorot_uniform\",\r\n",
        "                         input_length=1, name = 'word_embedding')(word_input)\r\n",
        "word_output = Reshape((embed_size, ))(word_embed)\r\n",
        "word_model = Model(word_input, word_output)\r\n",
        "\r\n",
        "print('word_model: \\n', word_model.summary())\r\n",
        "context_input = Input(shape = (1,))\r\n",
        "context_embed = Embedding(vocab_size, embed_size,\r\n",
        "                  embeddings_initializer=\"glorot_uniform\",\r\n",
        "                  input_length=1, name = 'context_embedding')(context_input)\r\n",
        "context_output = Reshape((embed_size,))(context_embed)\r\n",
        "context_model = Model(context_input, context_output)\r\n",
        "print('context_model: \\n', context_model.summary())\r\n",
        "\r\n",
        "concate = dot([word_output, context_output], axes = -1)\r\n",
        "dense = Dense(1, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\")(concate)\r\n",
        "model = Model(inputs = [word_input, context_input], outputs = dense)\r\n",
        "model.compile(loss=\"mean_squared_error\", optimizer=\"rmsprop\")\r\n",
        "\r\n",
        "# view model summary\r\n",
        "print('model merge word and context: \\n', model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 1)]               0         \n",
            "_________________________________________________________________\n",
            "word_embedding (Embedding)   (None, 1, 100)            1274600   \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 100)               0         \n",
            "=================================================================\n",
            "Total params: 1,274,600\n",
            "Trainable params: 1,274,600\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "word_model: \n",
            " None\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 1)]               0         \n",
            "_________________________________________________________________\n",
            "context_embedding (Embedding (None, 1, 100)            1274600   \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 100)               0         \n",
            "=================================================================\n",
            "Total params: 1,274,600\n",
            "Trainable params: 1,274,600\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "context_model: \n",
            " None\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
            "__________________________________________________________________________________________________\n",
            "word_embedding (Embedding)      (None, 1, 100)       1274600     input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "context_embedding (Embedding)   (None, 1, 100)       1274600     input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "reshape (Reshape)               (None, 100)          0           word_embedding[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 100)          0           context_embedding[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "dot (Dot)                       (None, 1)            0           reshape[0][0]                    \n",
            "                                                                 reshape_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_4 (Dense)                 (None, 1)            2           dot[0][0]                        \n",
            "==================================================================================================\n",
            "Total params: 2,549,202\n",
            "Trainable params: 2,549,202\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "model merge word and context: \n",
            " None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEFgylDsUBGV"
      },
      "source": [
        "## Biểu diễn kiến trúc của mô hình"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 709
        },
        "id": "KLPIOdL8UH3W",
        "outputId": "de81f5e1-d655-41ae-d8f5-a07564cf3895"
      },
      "source": [
        "# visualize model structure\r\n",
        "from IPython.display import SVG\r\n",
        "from keras.utils.vis_utils import model_to_dot\r\n",
        "\r\n",
        "SVG(model_to_dot(model, show_shapes=True, show_layer_names=False, \r\n",
        "                 rankdir='TB').create(prog='dot', format='svg'))\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.SVG object>"
            ],
            "image/svg+xml": "<svg height=\"516pt\" viewBox=\"0.00 0.00 514.00 387.00\" width=\"685pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n<g class=\"graph\" id=\"graph0\" transform=\"scale(1.3333 1.3333) rotate(0) translate(4 383)\">\n<title>G</title>\n<polygon fill=\"#ffffff\" points=\"-4,4 -4,-383 510,-383 510,4 -4,4\" stroke=\"transparent\"/>\n<!-- 140001925569104 -->\n<g class=\"node\" id=\"node1\">\n<title>140001925569104</title>\n<polygon fill=\"none\" points=\"12.5,-332.5 12.5,-378.5 231.5,-378.5 231.5,-332.5 12.5,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"52.5\" y=\"-351.8\">InputLayer</text>\n<polyline fill=\"none\" points=\"92.5,-332.5 92.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"121.5\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"92.5,-355.5 150.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"121.5\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"150.5,-332.5 150.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"191\" y=\"-363.3\">[(None, 1)]</text>\n<polyline fill=\"none\" points=\"150.5,-355.5 231.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"191\" y=\"-340.3\">[(None, 1)]</text>\n</g>\n<!-- 140001926126288 -->\n<g class=\"node\" id=\"node3\">\n<title>140001926126288</title>\n<polygon fill=\"none\" points=\"0,-249.5 0,-295.5 244,-295.5 244,-249.5 0,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"42\" y=\"-268.8\">Embedding</text>\n<polyline fill=\"none\" points=\"84,-249.5 84,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"113\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"84,-272.5 142,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"113\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"142,-249.5 142,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193\" y=\"-280.3\">(None, 1)</text>\n<polyline fill=\"none\" points=\"142,-272.5 244,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"193\" y=\"-257.3\">(None, 1, 100)</text>\n</g>\n<!-- 140001925569104&#45;&gt;140001926126288 -->\n<g class=\"edge\" id=\"edge1\">\n<title>140001925569104-&gt;140001926126288</title>\n<path d=\"M122,-332.3799C122,-324.1745 122,-314.7679 122,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"125.5001,-305.784 122,-295.784 118.5001,-305.784 125.5001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140001926934416 -->\n<g class=\"node\" id=\"node2\">\n<title>140001926934416</title>\n<polygon fill=\"none\" points=\"274.5,-332.5 274.5,-378.5 493.5,-378.5 493.5,-332.5 274.5,-332.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"314.5\" y=\"-351.8\">InputLayer</text>\n<polyline fill=\"none\" points=\"354.5,-332.5 354.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"383.5\" y=\"-363.3\">input:</text>\n<polyline fill=\"none\" points=\"354.5,-355.5 412.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"383.5\" y=\"-340.3\">output:</text>\n<polyline fill=\"none\" points=\"412.5,-332.5 412.5,-378.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"453\" y=\"-363.3\">[(None, 1)]</text>\n<polyline fill=\"none\" points=\"412.5,-355.5 493.5,-355.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"453\" y=\"-340.3\">[(None, 1)]</text>\n</g>\n<!-- 140001929229712 -->\n<g class=\"node\" id=\"node4\">\n<title>140001929229712</title>\n<polygon fill=\"none\" points=\"262,-249.5 262,-295.5 506,-295.5 506,-249.5 262,-249.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"304\" y=\"-268.8\">Embedding</text>\n<polyline fill=\"none\" points=\"346,-249.5 346,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"375\" y=\"-280.3\">input:</text>\n<polyline fill=\"none\" points=\"346,-272.5 404,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"375\" y=\"-257.3\">output:</text>\n<polyline fill=\"none\" points=\"404,-249.5 404,-295.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"455\" y=\"-280.3\">(None, 1)</text>\n<polyline fill=\"none\" points=\"404,-272.5 506,-272.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"455\" y=\"-257.3\">(None, 1, 100)</text>\n</g>\n<!-- 140001926934416&#45;&gt;140001929229712 -->\n<g class=\"edge\" id=\"edge2\">\n<title>140001926934416-&gt;140001929229712</title>\n<path d=\"M384,-332.3799C384,-324.1745 384,-314.7679 384,-305.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"387.5001,-305.784 384,-295.784 380.5001,-305.784 387.5001,-305.784\" stroke=\"#000000\"/>\n</g>\n<!-- 140001926783120 -->\n<g class=\"node\" id=\"node5\">\n<title>140001926783120</title>\n<polygon fill=\"none\" points=\"18.5,-166.5 18.5,-212.5 243.5,-212.5 243.5,-166.5 18.5,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"51\" y=\"-185.8\">Reshape</text>\n<polyline fill=\"none\" points=\"83.5,-166.5 83.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"112.5\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"83.5,-189.5 141.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"112.5\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"141.5,-166.5 141.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"192.5\" y=\"-197.3\">(None, 1, 100)</text>\n<polyline fill=\"none\" points=\"141.5,-189.5 243.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"192.5\" y=\"-174.3\">(None, 100)</text>\n</g>\n<!-- 140001926126288&#45;&gt;140001926783120 -->\n<g class=\"edge\" id=\"edge3\">\n<title>140001926126288-&gt;140001926783120</title>\n<path d=\"M124.507,-249.3799C125.3967,-241.1745 126.4167,-231.7679 127.3806,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"130.8767,-223.1031 128.4752,-212.784 123.9175,-222.3484 130.8767,-223.1031\" stroke=\"#000000\"/>\n</g>\n<!-- 140001926240912 -->\n<g class=\"node\" id=\"node6\">\n<title>140001926240912</title>\n<polygon fill=\"none\" points=\"266.5,-166.5 266.5,-212.5 491.5,-212.5 491.5,-166.5 266.5,-166.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"299\" y=\"-185.8\">Reshape</text>\n<polyline fill=\"none\" points=\"331.5,-166.5 331.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"360.5\" y=\"-197.3\">input:</text>\n<polyline fill=\"none\" points=\"331.5,-189.5 389.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"360.5\" y=\"-174.3\">output:</text>\n<polyline fill=\"none\" points=\"389.5,-166.5 389.5,-212.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"440.5\" y=\"-197.3\">(None, 1, 100)</text>\n<polyline fill=\"none\" points=\"389.5,-189.5 491.5,-189.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"440.5\" y=\"-174.3\">(None, 100)</text>\n</g>\n<!-- 140001929229712&#45;&gt;140001926240912 -->\n<g class=\"edge\" id=\"edge4\">\n<title>140001929229712-&gt;140001926240912</title>\n<path d=\"M382.6072,-249.3799C382.1129,-241.1745 381.5463,-231.7679 381.0108,-222.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"384.4977,-222.5554 380.4027,-212.784 377.5104,-222.9764 384.4977,-222.5554\" stroke=\"#000000\"/>\n</g>\n<!-- 140001925582992 -->\n<g class=\"node\" id=\"node7\">\n<title>140001925582992</title>\n<polygon fill=\"none\" points=\"117,-83.5 117,-129.5 387,-129.5 387,-83.5 117,-83.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"136\" y=\"-102.8\">Dot</text>\n<polyline fill=\"none\" points=\"155,-83.5 155,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"184\" y=\"-114.3\">input:</text>\n<polyline fill=\"none\" points=\"155,-106.5 213,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"184\" y=\"-91.3\">output:</text>\n<polyline fill=\"none\" points=\"213,-83.5 213,-129.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-114.3\">[(None, 100), (None, 100)]</text>\n<polyline fill=\"none\" points=\"213,-106.5 387,-106.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"300\" y=\"-91.3\">(None, 1)</text>\n</g>\n<!-- 140001926783120&#45;&gt;140001925582992 -->\n<g class=\"edge\" id=\"edge5\">\n<title>140001926783120-&gt;140001925582992</title>\n<path d=\"M164.7052,-166.3799C178.6176,-156.8367 194.8988,-145.6686 209.6073,-135.5793\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"211.7893,-138.3269 218.0558,-129.784 207.8296,-132.5544 211.7893,-138.3269\" stroke=\"#000000\"/>\n</g>\n<!-- 140001926240912&#45;&gt;140001925582992 -->\n<g class=\"edge\" id=\"edge6\">\n<title>140001926240912-&gt;140001925582992</title>\n<path d=\"M343.6234,-166.3799C328.8848,-156.7475 311.613,-145.4597 296.0625,-135.2967\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"297.913,-132.325 287.6273,-129.784 294.0834,-138.1846 297.913,-132.325\" stroke=\"#000000\"/>\n</g>\n<!-- 140001925957264 -->\n<g class=\"node\" id=\"node8\">\n<title>140001925957264</title>\n<polygon fill=\"none\" points=\"161,-.5 161,-46.5 343,-46.5 343,-.5 161,-.5\" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"187\" y=\"-19.8\">Dense</text>\n<polyline fill=\"none\" points=\"213,-.5 213,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242\" y=\"-31.3\">input:</text>\n<polyline fill=\"none\" points=\"213,-23.5 271,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"242\" y=\"-8.3\">output:</text>\n<polyline fill=\"none\" points=\"271,-.5 271,-46.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"307\" y=\"-31.3\">(None, 1)</text>\n<polyline fill=\"none\" points=\"271,-23.5 343,-23.5 \" stroke=\"#000000\"/>\n<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"307\" y=\"-8.3\">(None, 1)</text>\n</g>\n<!-- 140001925582992&#45;&gt;140001925957264 -->\n<g class=\"edge\" id=\"edge7\">\n<title>140001925582992-&gt;140001925957264</title>\n<path d=\"M252,-83.3799C252,-75.1745 252,-65.7679 252,-56.8786\" fill=\"none\" stroke=\"#000000\"/>\n<polygon fill=\"#000000\" points=\"255.5001,-56.784 252,-46.784 248.5001,-56.784 255.5001,-56.784\" stroke=\"#000000\"/>\n</g>\n</g>\n</svg>"
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NjHIFl4DUPRs"
      },
      "source": [
        "## Bước 3: Huấn luyện mô hình.\r\n",
        "\r\n",
        "Để cho nhanh ta sẽ training trên 100 skip_grams đầu tiên."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9xJe7uyUVw-",
        "outputId": "c3b52f95-4f38-4435-b2b9-f76dbe7d9fe1"
      },
      "source": [
        "for epoch in range(1, 6):\r\n",
        "    loss = 0\r\n",
        "    for i, elem in enumerate(skip_grams[:100]):\r\n",
        "        pair_first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\r\n",
        "        pair_second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\r\n",
        "        labels = np.array(elem[1], dtype='int32')\r\n",
        "        X = [pair_first_elem, pair_second_elem]\r\n",
        "        Y = labels\r\n",
        "        if i % 500 == 0:\r\n",
        "            print('Processed {} (skip_first, skip_second, relevance) pairs'.format(i))\r\n",
        "        loss += model.train_on_batch(X,Y)  \r\n",
        "\r\n",
        "    print('Epoch:', epoch, 'Loss:', loss)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 1 Loss: 24.906560748815536\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 2 Loss: 23.876232892274857\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 3 Loss: 21.584625020623207\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 4 Loss: 18.76884099841118\n",
            "Processed 0 (skip_first, skip_second, relevance) pairs\n",
            "Epoch: 5 Loss: 15.90737385302782\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epYVwj9FUdev"
      },
      "source": [
        "## Bước 4: Trích xuất ra véc tơ nhúng ở layer đầu tiên."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "id": "JnFpepXbUfrm",
        "outputId": "3be5b60d-e4d6-4420-f66d-8e7e1b36597c"
      },
      "source": [
        "import pandas as pd\r\n",
        "\r\n",
        "word_embedding_layer = model.get_layer('word_embedding')\r\n",
        "weights = word_embedding_layer.get_weights()[0]\r\n",
        "\r\n",
        "print(weights.shape)\r\n",
        "pd.DataFrame(weights, index=id2word.values()).head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12746, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "      <th>78</th>\n",
              "      <th>79</th>\n",
              "      <th>80</th>\n",
              "      <th>81</th>\n",
              "      <th>82</th>\n",
              "      <th>83</th>\n",
              "      <th>84</th>\n",
              "      <th>85</th>\n",
              "      <th>86</th>\n",
              "      <th>87</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>the</th>\n",
              "      <td>-0.012044</td>\n",
              "      <td>0.019198</td>\n",
              "      <td>0.007073</td>\n",
              "      <td>-0.013681</td>\n",
              "      <td>0.015124</td>\n",
              "      <td>-0.007346</td>\n",
              "      <td>0.017240</td>\n",
              "      <td>0.018487</td>\n",
              "      <td>-0.014790</td>\n",
              "      <td>0.011445</td>\n",
              "      <td>0.006384</td>\n",
              "      <td>0.018480</td>\n",
              "      <td>0.006845</td>\n",
              "      <td>0.006080</td>\n",
              "      <td>0.013151</td>\n",
              "      <td>-0.003769</td>\n",
              "      <td>-0.016630</td>\n",
              "      <td>0.014412</td>\n",
              "      <td>-0.021564</td>\n",
              "      <td>-0.019722</td>\n",
              "      <td>0.013694</td>\n",
              "      <td>-0.011118</td>\n",
              "      <td>-0.014382</td>\n",
              "      <td>0.017908</td>\n",
              "      <td>0.012392</td>\n",
              "      <td>-0.018910</td>\n",
              "      <td>-0.004939</td>\n",
              "      <td>0.007581</td>\n",
              "      <td>0.015749</td>\n",
              "      <td>-0.005116</td>\n",
              "      <td>0.005769</td>\n",
              "      <td>-0.009105</td>\n",
              "      <td>-0.003811</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>0.004242</td>\n",
              "      <td>0.015081</td>\n",
              "      <td>0.015593</td>\n",
              "      <td>-0.003927</td>\n",
              "      <td>-0.017008</td>\n",
              "      <td>-0.004584</td>\n",
              "      <td>...</td>\n",
              "      <td>0.017533</td>\n",
              "      <td>0.000488</td>\n",
              "      <td>0.009680</td>\n",
              "      <td>0.013766</td>\n",
              "      <td>0.018690</td>\n",
              "      <td>0.016383</td>\n",
              "      <td>0.005604</td>\n",
              "      <td>0.020443</td>\n",
              "      <td>-0.014396</td>\n",
              "      <td>0.009194</td>\n",
              "      <td>-0.010337</td>\n",
              "      <td>0.021327</td>\n",
              "      <td>0.011156</td>\n",
              "      <td>-0.014086</td>\n",
              "      <td>0.014528</td>\n",
              "      <td>-0.009188</td>\n",
              "      <td>-0.004807</td>\n",
              "      <td>-0.014238</td>\n",
              "      <td>0.000959</td>\n",
              "      <td>-0.005866</td>\n",
              "      <td>-0.002730</td>\n",
              "      <td>0.001932</td>\n",
              "      <td>-0.000844</td>\n",
              "      <td>-0.006053</td>\n",
              "      <td>-0.004763</td>\n",
              "      <td>-0.018170</td>\n",
              "      <td>0.021163</td>\n",
              "      <td>0.018995</td>\n",
              "      <td>0.002935</td>\n",
              "      <td>-0.014240</td>\n",
              "      <td>0.006210</td>\n",
              "      <td>-0.013202</td>\n",
              "      <td>0.020816</td>\n",
              "      <td>-0.008717</td>\n",
              "      <td>0.014306</td>\n",
              "      <td>-0.021592</td>\n",
              "      <td>-0.005027</td>\n",
              "      <td>-0.013784</td>\n",
              "      <td>0.007823</td>\n",
              "      <td>-0.002134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>and</th>\n",
              "      <td>0.397865</td>\n",
              "      <td>-0.377752</td>\n",
              "      <td>-0.341829</td>\n",
              "      <td>0.353048</td>\n",
              "      <td>-0.326999</td>\n",
              "      <td>0.285504</td>\n",
              "      <td>0.327806</td>\n",
              "      <td>0.387368</td>\n",
              "      <td>0.398001</td>\n",
              "      <td>0.327070</td>\n",
              "      <td>0.362766</td>\n",
              "      <td>0.349497</td>\n",
              "      <td>0.181637</td>\n",
              "      <td>-0.372802</td>\n",
              "      <td>-0.384828</td>\n",
              "      <td>0.397169</td>\n",
              "      <td>0.339491</td>\n",
              "      <td>-0.366489</td>\n",
              "      <td>0.386026</td>\n",
              "      <td>-0.406289</td>\n",
              "      <td>0.388367</td>\n",
              "      <td>-0.373759</td>\n",
              "      <td>0.106662</td>\n",
              "      <td>-0.383491</td>\n",
              "      <td>-0.324158</td>\n",
              "      <td>-0.350180</td>\n",
              "      <td>-0.402786</td>\n",
              "      <td>0.401596</td>\n",
              "      <td>0.384007</td>\n",
              "      <td>0.386818</td>\n",
              "      <td>0.395440</td>\n",
              "      <td>-0.382734</td>\n",
              "      <td>0.394118</td>\n",
              "      <td>0.392931</td>\n",
              "      <td>0.346130</td>\n",
              "      <td>0.382741</td>\n",
              "      <td>-0.376507</td>\n",
              "      <td>0.369230</td>\n",
              "      <td>-0.393327</td>\n",
              "      <td>0.327388</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.397734</td>\n",
              "      <td>-0.363275</td>\n",
              "      <td>-0.399981</td>\n",
              "      <td>-0.360737</td>\n",
              "      <td>0.365701</td>\n",
              "      <td>-0.374867</td>\n",
              "      <td>0.370131</td>\n",
              "      <td>-0.391806</td>\n",
              "      <td>0.380814</td>\n",
              "      <td>0.382974</td>\n",
              "      <td>-0.386416</td>\n",
              "      <td>-0.380167</td>\n",
              "      <td>0.385971</td>\n",
              "      <td>-0.338481</td>\n",
              "      <td>0.360552</td>\n",
              "      <td>0.366449</td>\n",
              "      <td>-0.345355</td>\n",
              "      <td>0.377054</td>\n",
              "      <td>0.378908</td>\n",
              "      <td>-0.395801</td>\n",
              "      <td>0.399568</td>\n",
              "      <td>0.386298</td>\n",
              "      <td>-0.282107</td>\n",
              "      <td>0.290844</td>\n",
              "      <td>-0.387824</td>\n",
              "      <td>0.383898</td>\n",
              "      <td>-0.381582</td>\n",
              "      <td>0.405954</td>\n",
              "      <td>-0.383009</td>\n",
              "      <td>0.394318</td>\n",
              "      <td>-0.397350</td>\n",
              "      <td>-0.386760</td>\n",
              "      <td>0.367012</td>\n",
              "      <td>0.374787</td>\n",
              "      <td>-0.062901</td>\n",
              "      <td>-0.380691</td>\n",
              "      <td>-0.349140</td>\n",
              "      <td>0.383632</td>\n",
              "      <td>0.415055</td>\n",
              "      <td>0.390777</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>of</th>\n",
              "      <td>0.407229</td>\n",
              "      <td>0.058817</td>\n",
              "      <td>0.341515</td>\n",
              "      <td>0.418108</td>\n",
              "      <td>-0.416203</td>\n",
              "      <td>0.406274</td>\n",
              "      <td>0.408926</td>\n",
              "      <td>0.351413</td>\n",
              "      <td>0.420778</td>\n",
              "      <td>0.401644</td>\n",
              "      <td>0.370293</td>\n",
              "      <td>-0.275674</td>\n",
              "      <td>0.395392</td>\n",
              "      <td>-0.412598</td>\n",
              "      <td>-0.375126</td>\n",
              "      <td>0.299669</td>\n",
              "      <td>0.419740</td>\n",
              "      <td>-0.408397</td>\n",
              "      <td>0.396304</td>\n",
              "      <td>-0.360372</td>\n",
              "      <td>0.399887</td>\n",
              "      <td>-0.416993</td>\n",
              "      <td>-0.388500</td>\n",
              "      <td>-0.430070</td>\n",
              "      <td>-0.426729</td>\n",
              "      <td>-0.433804</td>\n",
              "      <td>-0.438356</td>\n",
              "      <td>0.435881</td>\n",
              "      <td>0.429687</td>\n",
              "      <td>0.440018</td>\n",
              "      <td>0.433936</td>\n",
              "      <td>-0.414249</td>\n",
              "      <td>0.345637</td>\n",
              "      <td>0.447333</td>\n",
              "      <td>0.204712</td>\n",
              "      <td>-0.201109</td>\n",
              "      <td>-0.446582</td>\n",
              "      <td>0.441781</td>\n",
              "      <td>-0.427165</td>\n",
              "      <td>0.405514</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.438127</td>\n",
              "      <td>-0.382004</td>\n",
              "      <td>-0.420848</td>\n",
              "      <td>-0.167617</td>\n",
              "      <td>0.384356</td>\n",
              "      <td>-0.442676</td>\n",
              "      <td>0.422957</td>\n",
              "      <td>0.286465</td>\n",
              "      <td>0.388234</td>\n",
              "      <td>0.358923</td>\n",
              "      <td>-0.439020</td>\n",
              "      <td>-0.420663</td>\n",
              "      <td>0.329519</td>\n",
              "      <td>-0.316222</td>\n",
              "      <td>0.427519</td>\n",
              "      <td>0.409289</td>\n",
              "      <td>-0.403531</td>\n",
              "      <td>0.419849</td>\n",
              "      <td>0.433559</td>\n",
              "      <td>-0.353053</td>\n",
              "      <td>0.361016</td>\n",
              "      <td>0.389113</td>\n",
              "      <td>-0.411289</td>\n",
              "      <td>0.386415</td>\n",
              "      <td>-0.440109</td>\n",
              "      <td>0.349841</td>\n",
              "      <td>-0.396004</td>\n",
              "      <td>0.445101</td>\n",
              "      <td>-0.374906</td>\n",
              "      <td>0.433711</td>\n",
              "      <td>-0.407264</td>\n",
              "      <td>-0.430501</td>\n",
              "      <td>0.436257</td>\n",
              "      <td>-0.331304</td>\n",
              "      <td>0.417410</td>\n",
              "      <td>-0.423647</td>\n",
              "      <td>0.357565</td>\n",
              "      <td>0.353301</td>\n",
              "      <td>0.369371</td>\n",
              "      <td>0.439020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>to</th>\n",
              "      <td>0.309908</td>\n",
              "      <td>0.099762</td>\n",
              "      <td>0.264870</td>\n",
              "      <td>0.303282</td>\n",
              "      <td>-0.305559</td>\n",
              "      <td>0.262531</td>\n",
              "      <td>0.275440</td>\n",
              "      <td>0.269295</td>\n",
              "      <td>0.286492</td>\n",
              "      <td>0.320941</td>\n",
              "      <td>0.236706</td>\n",
              "      <td>-0.265686</td>\n",
              "      <td>0.282884</td>\n",
              "      <td>-0.305714</td>\n",
              "      <td>-0.268608</td>\n",
              "      <td>0.279896</td>\n",
              "      <td>0.278158</td>\n",
              "      <td>-0.301526</td>\n",
              "      <td>0.270075</td>\n",
              "      <td>-0.254989</td>\n",
              "      <td>0.303287</td>\n",
              "      <td>-0.246401</td>\n",
              "      <td>-0.084537</td>\n",
              "      <td>-0.320314</td>\n",
              "      <td>-0.235564</td>\n",
              "      <td>-0.305323</td>\n",
              "      <td>-0.295779</td>\n",
              "      <td>0.289173</td>\n",
              "      <td>0.302411</td>\n",
              "      <td>0.293852</td>\n",
              "      <td>0.307516</td>\n",
              "      <td>-0.267629</td>\n",
              "      <td>0.293652</td>\n",
              "      <td>0.327314</td>\n",
              "      <td>-0.258904</td>\n",
              "      <td>-0.262256</td>\n",
              "      <td>-0.315829</td>\n",
              "      <td>0.280034</td>\n",
              "      <td>-0.289997</td>\n",
              "      <td>0.316455</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.268506</td>\n",
              "      <td>-0.290050</td>\n",
              "      <td>-0.316989</td>\n",
              "      <td>0.146863</td>\n",
              "      <td>0.313699</td>\n",
              "      <td>-0.275477</td>\n",
              "      <td>0.290026</td>\n",
              "      <td>0.104249</td>\n",
              "      <td>0.222247</td>\n",
              "      <td>-0.008668</td>\n",
              "      <td>-0.317426</td>\n",
              "      <td>-0.299863</td>\n",
              "      <td>0.113492</td>\n",
              "      <td>-0.214120</td>\n",
              "      <td>0.284263</td>\n",
              "      <td>0.290178</td>\n",
              "      <td>-0.257389</td>\n",
              "      <td>0.297467</td>\n",
              "      <td>0.302484</td>\n",
              "      <td>-0.304517</td>\n",
              "      <td>0.212612</td>\n",
              "      <td>0.304048</td>\n",
              "      <td>-0.297412</td>\n",
              "      <td>0.275650</td>\n",
              "      <td>-0.314645</td>\n",
              "      <td>0.295395</td>\n",
              "      <td>-0.299332</td>\n",
              "      <td>0.311467</td>\n",
              "      <td>-0.293368</td>\n",
              "      <td>0.294285</td>\n",
              "      <td>-0.286672</td>\n",
              "      <td>-0.307619</td>\n",
              "      <td>0.315744</td>\n",
              "      <td>-0.182492</td>\n",
              "      <td>0.294696</td>\n",
              "      <td>-0.296565</td>\n",
              "      <td>0.246236</td>\n",
              "      <td>0.289303</td>\n",
              "      <td>0.286713</td>\n",
              "      <td>0.299939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>that</th>\n",
              "      <td>0.183322</td>\n",
              "      <td>0.022625</td>\n",
              "      <td>0.029729</td>\n",
              "      <td>0.157262</td>\n",
              "      <td>-0.190340</td>\n",
              "      <td>0.179988</td>\n",
              "      <td>0.194140</td>\n",
              "      <td>0.151936</td>\n",
              "      <td>0.180073</td>\n",
              "      <td>0.181102</td>\n",
              "      <td>0.121072</td>\n",
              "      <td>-0.077323</td>\n",
              "      <td>0.174236</td>\n",
              "      <td>-0.168686</td>\n",
              "      <td>-0.175827</td>\n",
              "      <td>0.157195</td>\n",
              "      <td>0.185889</td>\n",
              "      <td>-0.162311</td>\n",
              "      <td>0.164476</td>\n",
              "      <td>-0.171810</td>\n",
              "      <td>0.181877</td>\n",
              "      <td>-0.169797</td>\n",
              "      <td>0.124561</td>\n",
              "      <td>-0.196202</td>\n",
              "      <td>-0.180244</td>\n",
              "      <td>-0.162839</td>\n",
              "      <td>-0.183264</td>\n",
              "      <td>0.188229</td>\n",
              "      <td>0.195891</td>\n",
              "      <td>0.173805</td>\n",
              "      <td>0.168614</td>\n",
              "      <td>-0.176443</td>\n",
              "      <td>0.144065</td>\n",
              "      <td>0.171897</td>\n",
              "      <td>0.052628</td>\n",
              "      <td>-0.068634</td>\n",
              "      <td>-0.182075</td>\n",
              "      <td>0.173160</td>\n",
              "      <td>-0.152627</td>\n",
              "      <td>0.155885</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.190550</td>\n",
              "      <td>-0.155113</td>\n",
              "      <td>-0.186377</td>\n",
              "      <td>-0.148630</td>\n",
              "      <td>0.199291</td>\n",
              "      <td>-0.171765</td>\n",
              "      <td>0.134589</td>\n",
              "      <td>0.071497</td>\n",
              "      <td>0.170886</td>\n",
              "      <td>0.117690</td>\n",
              "      <td>-0.173805</td>\n",
              "      <td>-0.151083</td>\n",
              "      <td>0.113567</td>\n",
              "      <td>-0.129593</td>\n",
              "      <td>0.165616</td>\n",
              "      <td>0.182173</td>\n",
              "      <td>-0.177386</td>\n",
              "      <td>0.196794</td>\n",
              "      <td>0.186126</td>\n",
              "      <td>-0.191022</td>\n",
              "      <td>0.127322</td>\n",
              "      <td>0.167761</td>\n",
              "      <td>-0.156348</td>\n",
              "      <td>0.141336</td>\n",
              "      <td>-0.168029</td>\n",
              "      <td>0.124332</td>\n",
              "      <td>-0.177675</td>\n",
              "      <td>0.184972</td>\n",
              "      <td>-0.169538</td>\n",
              "      <td>0.166816</td>\n",
              "      <td>-0.181075</td>\n",
              "      <td>-0.158585</td>\n",
              "      <td>0.175424</td>\n",
              "      <td>-0.139187</td>\n",
              "      <td>0.131267</td>\n",
              "      <td>-0.161513</td>\n",
              "      <td>0.146773</td>\n",
              "      <td>0.158480</td>\n",
              "      <td>0.168049</td>\n",
              "      <td>0.174801</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 100 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2   ...        97        98        99\n",
              "the  -0.012044  0.019198  0.007073  ... -0.013784  0.007823 -0.002134\n",
              "and   0.397865 -0.377752 -0.341829  ...  0.383632  0.415055  0.390777\n",
              "of    0.407229  0.058817  0.341515  ...  0.353301  0.369371  0.439020\n",
              "to    0.309908  0.099762  0.264870  ...  0.289303  0.286713  0.299939\n",
              "that  0.183322  0.022625  0.029729  ...  0.158480  0.168049  0.174801\n",
              "\n",
              "[5 rows x 100 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pH7w3hDXUldV"
      },
      "source": [
        "## Tìm các từ gần nghĩa nhất với 2 từ ['egypt', 'king'] dựa trên khoảng cách euclidean."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WAsI2quIUxDG",
        "outputId": "767cf9f3-f3d8-443c-bebf-ba9a7d2a09a8"
      },
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\r\n",
        "\r\n",
        "distance_matrix = euclidean_distances(weights)\r\n",
        "print(distance_matrix.shape)\r\n",
        "\r\n",
        "similar_words = {search_term: [id2word[idx] for idx in distance_matrix[word2id[search_term]-1].argsort()[1:6]+1] \r\n",
        "                   for search_term in ['egypt', 'king']}\r\n",
        "\r\n",
        "similar_words"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(12746, 12746)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'egypt': ['come', 'from', 'earth', 'hearken', 'nor'],\n",
              " 'king': ['by', 'out', 'son', 'lord', 'these']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abPzwJ1GU4Ow"
      },
      "source": [
        "# Biểu diễn t-SNE\r\n",
        "t-SNE là một thuật toán giảm chiều dữ liệu dimensionality reduction rất hiệu quả. Thông thường đối với những véc tơ nhiều hơn 3 chiều chúng ta sẽ tìm cách giảm chúng về 2 hoặc 3 chiều bằng thuật toán t-SNE và biểu diễn chúng trong không gian để nhận biết mối liên hệ, tính chất.\r\n",
        "\r\n",
        "Tiếp theo chúng ta sẽ biểu diễn các từ trong không gian 2 chiều dựa trên thuật toán t-SNE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 500
        },
        "id": "xqOlolDSU7ie",
        "outputId": "40b36f2f-5527-4b6c-dd95-a8727de21f41"
      },
      "source": [
        "from sklearn.manifold import TSNE\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "words = sum([[k] + v for k, v in similar_words.items()], [])\r\n",
        "words_ids = [word2id[w] for w in words]\r\n",
        "word_vectors = np.array([weights[idx] for idx in words_ids])\r\n",
        "print('Total words:', len(words), '\\tWord Embedding shapes:', word_vectors.shape)\r\n",
        "\r\n",
        "tsne = TSNE(n_components=2, random_state=0, n_iter=10000, perplexity=3)\r\n",
        "np.set_printoptions(suppress=True)\r\n",
        "T = tsne.fit_transform(word_vectors)\r\n",
        "labels = words\r\n",
        "\r\n",
        "plt.figure(figsize=(14, 8))\r\n",
        "plt.scatter(T[:, 0], T[:, 1], c='steelblue', edgecolors='k')\r\n",
        "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\r\n",
        "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total words: 12 \tWord Embedding shapes: (12, 100)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1008x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3Ue3RYdVHQ2"
      },
      "source": [
        "# Sử dụng gensim cho mô hình word2vec\r\n",
        "\r\n",
        "Cách training trên chỉ sử dụng để chúng ta hiểu rõ cơ chế hoạt động của 2 phương pháp skip-grams và CBOW trong mô hình word2vec. Trên thực tế mô hình có thể được training trên gensim với chỉ 1 vài dòng rất đơn giản như sau. Trong đó có một số tham số quan trọng trong Word2Vec:\r\n",
        "\r\n",
        "1. size: Kích thước của ma trận nhúng.\r\n",
        "2. window: Kích thước cửa sổ được sử dụng để khởi tạo các n-gram.\r\n",
        "3. sg: Nhận 2 giá trị {0, 1}. Nếu là 0: phương pháp CBOW, nếu là 1: skip-grams.\r\n",
        "4. wokers: Số core CPU được huy động để huấn luyện. Càng nhiều core tốc độ huấn luyện càng nhanh.\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0s03KD_TVNfG",
        "outputId": "4efd51e3-c6ad-432e-d6a9-48ad241cee24"
      },
      "source": [
        "from gensim.models import Word2Vec\r\n",
        "# Training model với 1000 câu đầu tiên trong kinh thánh\r\n",
        "sentences = [[item.lower() for item in doc.split()] for doc in norm_bible[:1000]]\r\n",
        "model = Word2Vec(sentences, min_count = 1, size = 150, window = 10, sg = 1, workers = 8)\r\n",
        "model.train(sentences, total_examples = model.corpus_count, epochs = 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(209770, 336740)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mq4UKrnrV5hH",
        "outputId": "1ca3abaa-17b6-43f4-ff93-8c85a24dada0"
      },
      "source": [
        "# Lấy véc tơ biểu diễn của từ king\r\n",
        "print('embedding vector shape: ', model.wv['king'].shape)\r\n",
        "model.wv['king']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "embedding vector shape:  (150,)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.32769266,  0.03325617,  0.16099454, -0.43923625, -0.5774205 ,\n",
              "        0.32666007,  0.00803816,  0.01739315, -0.17381752,  0.47398144,\n",
              "        0.10426191, -0.30574593, -0.01257783,  0.45202482, -0.06158702,\n",
              "        0.40313822,  0.12463704,  0.09550165, -0.37712634,  0.2617265 ,\n",
              "       -0.30980474, -0.14957416, -0.3931157 ,  0.07375994,  0.41351643,\n",
              "       -0.19939345,  0.3283562 ,  0.16446988, -0.3300091 , -0.04926994,\n",
              "       -0.15286638,  0.19007151,  0.55644476,  0.1945966 ,  0.4461981 ,\n",
              "       -0.18473591,  0.22609885, -0.9014331 ,  0.01548504, -0.21673821,\n",
              "        0.10287414, -0.25499648,  0.01149429,  0.579976  ,  0.26826152,\n",
              "        0.31048036, -0.2894414 ,  0.15242869, -0.01445581, -0.22635837,\n",
              "        0.17343895,  0.03684635, -0.09589378,  0.2466696 ,  0.06005651,\n",
              "        0.18786697, -0.00684533, -0.07377088,  0.13694996,  0.00549853,\n",
              "       -0.26337558, -0.55286056,  0.70527333, -0.2520205 ,  0.01884417,\n",
              "       -0.02746831,  0.02288482,  0.32463688, -0.18272272, -0.15540704,\n",
              "       -0.21154991, -0.23531258,  0.11155473,  0.49225354,  0.12813924,\n",
              "        0.01358837, -0.0617293 ,  0.4091261 ,  0.16113189, -0.4794723 ,\n",
              "        0.21329418, -0.02377128,  0.17682534, -0.14040537, -0.10301828,\n",
              "        0.03362725, -0.1765054 , -0.00904407, -0.18431598, -0.09549105,\n",
              "        0.44000438,  0.07767139,  0.1663786 ,  0.1218497 , -0.06613931,\n",
              "        0.34376195, -0.04857645,  0.4457503 ,  0.02866266, -0.10288871,\n",
              "        0.09204753,  0.12405114,  0.1029912 ,  0.5908861 ,  0.5554401 ,\n",
              "       -0.3695465 ,  0.33683106,  0.32236347,  0.28179425, -0.11971387,\n",
              "       -0.02198073, -0.25243378, -0.10597385,  0.19987243, -0.2866279 ,\n",
              "        0.28595322, -0.03860004,  0.07965741,  0.12920153, -0.33358878,\n",
              "        0.01341007, -0.71531034,  0.00502061, -0.17091964, -0.20818426,\n",
              "       -0.09731632, -0.40887433,  0.3806525 , -0.03713652,  0.21474151,\n",
              "        0.19287506, -0.01612367, -0.26603043, -0.09373984, -0.24117365,\n",
              "       -0.06823396,  0.23413415, -0.4034562 ,  0.12902348, -0.2118021 ,\n",
              "       -0.2494434 ,  0.22739327,  0.24450633,  0.4273947 , -0.03291015,\n",
              "        0.02355326,  0.4220294 , -0.05326239,  0.20629998, -0.19923362],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GUMbC3x-V-tH",
        "outputId": "a698be46-25be-43c6-87c3-53416ab5db10"
      },
      "source": [
        "# Lấy các từ có mối liên hệ gần nhất với 1 từ dựa trên khoảng cách\r\n",
        "model.most_similar('king')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('admah', 0.869554877281189),\n",
              " ('elam', 0.8658518195152283),\n",
              " ('chedorlaomer', 0.8654919266700745),\n",
              " ('arioch', 0.8652955889701843),\n",
              " ('ellasar', 0.861585259437561),\n",
              " ('tidal', 0.8593940734863281),\n",
              " ('shinar', 0.8454746007919312),\n",
              " ('amraphel', 0.84051114320755),\n",
              " ('zeboiim', 0.8353822827339172),\n",
              " ('shinab', 0.8315096497535706)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdi-DzUcL7qd"
      },
      "source": [
        "# Mục mới"
      ]
    }
  ]
}